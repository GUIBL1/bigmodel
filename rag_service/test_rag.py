"""
ç®€åŒ–çš„RAGæµ‹è¯•å’Œå®‰è£…éªŒè¯
"""

def test_imports():
    """æµ‹è¯•æ‰€éœ€åŒ…çš„å¯¼å…¥"""
    print("ğŸ§ª æµ‹è¯•åŒ…å¯¼å…¥...")
    
    try:
        import flask
        print("âœ… Flaskå¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ Flaskå¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        import chromadb
        print("âœ… ChromaDBå¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ ChromaDBå¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        import sentence_transformers
        print("âœ… SentenceTransformerså¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ SentenceTransformerså¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        import torch
        print("âœ… PyTorchå¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ PyTorchå¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        import llama_index
        print("âœ… LlamaIndexå¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ LlamaIndexå¯¼å…¥å¤±è´¥: {e}")
        return False
    
    print("ğŸ‰ æ‰€æœ‰æ ¸å¿ƒåŒ…å¯¼å…¥æˆåŠŸï¼")
    return True

def test_ollama_connection():
    """æµ‹è¯•Ollamaè¿æ¥"""
    print("\nğŸ”— æµ‹è¯•Ollamaè¿æ¥...")
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… OllamaæœåŠ¡æ­£å¸¸è¿è¡Œ")
            models = response.json().get('models', [])
            if models:
                print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {[m['name'] for m in models]}")
            else:
                print("âš ï¸ æ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œè¯·è¿è¡Œ: ollama pull llama3.1")
            return True
        else:
            print(f"âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥Ollama: {e}")
        print("è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ: ollama serve")
        return False

def create_simple_rag():
    """åˆ›å»ºç®€åŒ–çš„RAGæœåŠ¡"""
    print("\nğŸ—ï¸ åˆ›å»ºç®€åŒ–RAGæœåŠ¡...")
    
    try:
        # åŸºæœ¬å¯¼å…¥æµ‹è¯•
        import chromadb
        from sentence_transformers import SentenceTransformer
        import os
        
        # æ£€æŸ¥ç½‘ç»œè¿æ¥
        import requests
        has_internet = False
        try:
            response = requests.get("https://huggingface.co", timeout=5)
            has_internet = response.status_code == 200
        except:
            has_internet = False
        
        print(f"ğŸŒ ç½‘ç»œçŠ¶æ€: {'å¯ç”¨' if has_internet else 'ç¦»çº¿'}")
        
        # å°è¯•ä½¿ç”¨æœ¬åœ°å¯ç”¨çš„åµŒå…¥æ¨¡å‹
        embed_model = None
        
        if has_internet:
            # æœ‰ç½‘ç»œæ—¶å°è¯•åœ¨çº¿æ¨¡å‹
            model_options = [
                'all-MiniLM-L6-v2',
                'paraphrase-MiniLM-L6-v2',
                'all-mpnet-base-v2'
            ]
            
            for model_name in model_options:
                try:
                    print(f"ğŸ” å°è¯•åŠ è½½åœ¨çº¿æ¨¡å‹: {model_name}")
                    cache_dir = "./embeddings_cache"
                    os.makedirs(cache_dir, exist_ok=True)
                    embed_model = SentenceTransformer(model_name, cache_folder=cache_dir)
                    print(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
                    break
                except Exception as e:
                    print(f"âš ï¸ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {str(e)[:100]}...")
                    continue
        
        if embed_model is None:
            print("ğŸ”„ åˆ‡æ¢åˆ°ç¦»çº¿æ¨¡å¼...")
            # ä½¿ç”¨scikit-learnçš„TfidfVectorizerä½œä¸ºç¦»çº¿æ›¿ä»£
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                from sklearn.metrics.pairwise import cosine_similarity
                import numpy as np
                
                print("âœ… ä½¿ç”¨ç¦»çº¿TF-IDFå‘é‡åŒ–å™¨")
                
                # åˆ›å»ºç®€å•çš„TF-IDFåµŒå…¥ç±»
                class OfflineEmbedding:
                    def __init__(self):
                        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
                        self.is_fitted = False
                    
                    def encode(self, texts):
                        if isinstance(texts, str):
                            texts = [texts]
                        
                        if not self.is_fitted:
                            vectors = self.vectorizer.fit_transform(texts)
                            self.is_fitted = True
                        else:
                            vectors = self.vectorizer.transform(texts)
                        
                        return vectors.toarray()
                
                embed_model = OfflineEmbedding()
                print("âœ… ç¦»çº¿åµŒå…¥æ¨¡å‹åˆ›å»ºæˆåŠŸ")
                
            except ImportError:
                print("âŒ æ— æ³•å¯¼å…¥scikit-learnï¼Œè¯·å®‰è£…:")
                print("pip install scikit-learn")
                return False
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        chroma_client = chromadb.PersistentClient(path="./test_vector_store")
        collection = chroma_client.get_or_create_collection("test_collection")
        print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ–‡æ¡£å¤„ç†
        test_docs = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯RAGåŠŸèƒ½ã€‚",
            "RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„AIæŠ€æœ¯ã€‚",
            "æœ¬ç³»ç»Ÿæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ŒåŒ…æ‹¬PDFã€Wordã€Excelç­‰ã€‚"
        ]
        
        # æ–‡æ¡£å‘é‡åŒ–
        print("ğŸ”„ æ­£åœ¨è¿›è¡Œæ–‡æ¡£å‘é‡åŒ–...")
        embeddings = embed_model.encode(test_docs)
        
        # æ¸…ç©ºç°æœ‰æ•°æ®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        try:
            collection.delete()
        except:
            pass
        
        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        collection.add(
            embeddings=embeddings.tolist(),
            documents=test_docs,
            ids=[f"doc_{i}" for i in range(len(test_docs))]
        )
        print("âœ… æµ‹è¯•æ–‡æ¡£å‘é‡åŒ–å’Œå­˜å‚¨æˆåŠŸ")
        
        # æµ‹è¯•æ£€ç´¢
        query = "ä»€ä¹ˆæ˜¯RAGï¼Ÿ"
        print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        query_embedding = embed_model.encode([query])
        
        results = collection.query(
            query_embeddings=query_embedding.tolist(),
            n_results=2
        )
        
        if results['documents'] and len(results['documents'][0]) > 0:
            print("âœ… æ–‡æ¡£æ£€ç´¢æµ‹è¯•æˆåŠŸ")
            print(f"ï¿½ æœ€ç›¸å…³ç»“æœ: {results['documents'][0][0]}")
            if len(results['distances'][0]) > 0:
                print(f"ğŸ¯ ç›¸ä¼¼åº¦åˆ†æ•°: {1 - results['distances'][0][0]:.3f}")
            return True
        else:
            print("âŒ æ–‡æ¡£æ£€ç´¢å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ RAGæœåŠ¡åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        print("ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ RAGæœåŠ¡å®‰è£…å’ŒåŠŸèƒ½éªŒè¯")
    print("=" * 50)
    
    # æµ‹è¯•å¯¼å…¥
    if not test_imports():
        print("\nâŒ åŒ…å¯¼å…¥æµ‹è¯•å¤±è´¥ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–")
        print("è¿è¡Œ: python install_deps.bat")
        exit(1)
    
    # æµ‹è¯•Ollama
    test_ollama_connection()
    
    # æµ‹è¯•åŸºæœ¬RAGåŠŸèƒ½
    if create_simple_rag():
        print("\nğŸ‰ åŸºæœ¬RAGåŠŸèƒ½éªŒè¯æˆåŠŸï¼")
        print("âœ¨ å¯ä»¥ç»§ç»­å¯åŠ¨å®Œæ•´çš„RAGæœåŠ¡")
    else:
        print("\nâŒ RAGåŠŸèƒ½éªŒè¯å¤±è´¥")
        print("è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•")
