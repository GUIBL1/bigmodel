"""
ç¦»çº¿RAGæµ‹è¯• - ä¸ä¾èµ–ç½‘ç»œä¸‹è½½æ¨¡å‹
ä½¿ç”¨ç®€å•çš„TF-IDFå‘é‡åŒ–ä½œä¸ºåµŒå…¥æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆ
"""

def test_offline_rag():
    """åˆ›å»ºå®Œå…¨ç¦»çº¿çš„RAGæœåŠ¡"""
    print("\nğŸ—ï¸ åˆ›å»ºç¦»çº¿RAGæœåŠ¡...")
    
    try:
        import chromadb
        import numpy as np
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        import jieba  # ä¸­æ–‡åˆ†è¯
        
        print("âœ… å¯¼å…¥ç¦»çº¿ä¾èµ–æˆåŠŸ")
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        chroma_client = chromadb.PersistentClient(path="./offline_vector_store")
        collection = chroma_client.get_or_create_collection("offline_test")
        print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ–‡æ¡£
        test_docs = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯RAGåŠŸèƒ½ã€‚RAGç³»ç»Ÿå¯ä»¥æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚",
            "RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„AIæŠ€æœ¯ã€‚å®ƒèƒ½æé«˜å›ç­”å‡†ç¡®æ€§ã€‚",
            "æœ¬ç³»ç»Ÿæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ŒåŒ…æ‹¬PDFã€Wordã€Excelç­‰ã€‚æ–‡æ¡£ä¼šè¢«è‡ªåŠ¨å¤„ç†å’Œç´¢å¼•ã€‚",
            "å‘é‡æ•°æ®åº“å­˜å‚¨æ–‡æ¡£çš„åµŒå…¥è¡¨ç¤ºï¼Œæ”¯æŒè¯­ä¹‰æœç´¢å’Œç›¸ä¼¼åº¦è®¡ç®—ã€‚",
            "å¤§è¯­è¨€æ¨¡å‹å¦‚GPTã€Llamaç­‰å¯ä»¥ç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”ï¼Œç»“åˆæ£€ç´¢ä¿¡æ¯æ›´å‡†ç¡®ã€‚"
        ]
        
        # ä½¿ç”¨TF-IDFä½œä¸ºç®€å•çš„å‘é‡åŒ–æ–¹æ³•
        print("ğŸ”„ ä½¿ç”¨TF-IDFè¿›è¡Œæ–‡æ¡£å‘é‡åŒ–...")
        
        # ä¸­æ–‡åˆ†è¯é¢„å¤„ç†
        def preprocess_text(text):
            # ä½¿ç”¨jiebaåˆ†è¯
            words = jieba.cut(text)
            return ' '.join(words)
        
        processed_docs = [preprocess_text(doc) for doc in test_docs]
        
        # åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
        vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        doc_vectors = vectorizer.fit_transform(processed_docs)
        
        # è½¬æ¢ä¸ºdense numpyæ•°ç»„
        doc_embeddings = doc_vectors.toarray()
        
        # æ¸…ç©ºç°æœ‰æ•°æ®
        try:
            collection.delete()
        except:
            pass
        
        # å­˜å‚¨åˆ°ChromaDB
        collection.add(
            embeddings=doc_embeddings.tolist(),
            documents=test_docs,
            metadatas=[{"doc_id": i, "length": len(doc)} for i, doc in enumerate(test_docs)],
            ids=[f"doc_{i}" for i in range(len(test_docs))]
        )
        print("âœ… æ–‡æ¡£å‘é‡åŒ–å’Œå­˜å‚¨æˆåŠŸ")
        
        # æµ‹è¯•æ£€ç´¢åŠŸèƒ½
        queries = [
            "ä»€ä¹ˆæ˜¯RAGï¼Ÿ",
            "æ”¯æŒå“ªäº›æ–‡æ¡£æ ¼å¼ï¼Ÿ",
            "å¦‚ä½•è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Ÿ"
        ]
        
        print("\nğŸ” æµ‹è¯•æ£€ç´¢åŠŸèƒ½:")
        for query in queries:
            print(f"\næŸ¥è¯¢: {query}")
            
            # æŸ¥è¯¢å‘é‡åŒ–
            processed_query = preprocess_text(query)
            query_vector = vectorizer.transform([processed_query]).toarray()
            
            # ä½¿ç”¨ChromaDBæŸ¥è¯¢
            results = collection.query(
                query_embeddings=query_vector.tolist(),
                n_results=2
            )
            
            if results['documents'] and len(results['documents'][0]) > 0:
                print(f"âœ… æœ€ç›¸å…³æ–‡æ¡£: {results['documents'][0][0][:100]}...")
                if results['distances'][0]:
                    similarity = 1 - results['distances'][0][0]
                    print(f"ğŸ¯ ç›¸ä¼¼åº¦: {similarity:.3f}")
            else:
                print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        
        return True
        
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
        print("è¯·å®‰è£…: pip install scikit-learn jieba")
        return False
    except Exception as e:
        print(f"âŒ ç¦»çº¿RAGæœåŠ¡åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_embedding():
    """æµ‹è¯•ç®€å•çš„å‘é‡åµŒå…¥æ–¹æ³•"""
    print("\nğŸ§ª æµ‹è¯•ç®€å•å‘é‡åµŒå…¥...")
    
    try:
        from sklearn.feature_extraction.text import CountVectorizer
        import numpy as np
        
        # ç®€å•çš„è¯è¢‹æ¨¡å‹æµ‹è¯•
        docs = ["è¿™æ˜¯æµ‹è¯•", "RAGæŠ€æœ¯å¾ˆå¥½", "å‘é‡æ£€ç´¢æœ‰æ•ˆ"]
        
        vectorizer = CountVectorizer()
        vectors = vectorizer.fit_transform(docs)
        
        print("âœ… ç®€å•å‘é‡åŒ–æˆåŠŸ")
        print(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {len(vectorizer.vocabulary_)}")
        print(f"ğŸ”¢ å‘é‡ç»´åº¦: {vectors.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å•åµŒå…¥æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ ç¦»çº¿RAGåŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•ç®€å•åµŒå…¥
    if test_simple_embedding():
        print("\nâœ¨ ç®€å•åµŒå…¥æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•ç¦»çº¿RAG
    if test_offline_rag():
        print("\nğŸ‰ ç¦»çº¿RAGåŠŸèƒ½éªŒè¯æˆåŠŸï¼")
        print("âœ¨ åŸºç¡€RAGåŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥ç»§ç»­é›†æˆ")
    else:
        print("\nâŒ ç¦»çº¿RAGåŠŸèƒ½éªŒè¯å¤±è´¥")
        print("æ­£åœ¨å®‰è£…ç¼ºå¤±çš„ä¾èµ–...")
        
        import subprocess
        try:
            subprocess.run(["pip", "install", "scikit-learn", "jieba"], check=True)
            print("âœ… ä¾èµ–å®‰è£…å®Œæˆï¼Œè¯·é‡æ–°è¿è¡Œæµ‹è¯•")
        except:
            print("âŒ è‡ªåŠ¨å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œ: pip install scikit-learn jieba")
